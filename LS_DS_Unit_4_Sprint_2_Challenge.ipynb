{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** Another term for \"nodes\", they recieve inputs and pass it on to the next set of nodes once a certain thershold is reached.\n",
    "- **Input Layer:** Also known as the visible layer, it is what the data interacts with directly. hence the name \"Inupt\"\n",
    "- **Hidden Layer:** No direct interaction, therefore there's no way to bring them out without the Input layer. They are hidden within the network\n",
    "- **Output Layer:** Known as the last nodes of the network, Their purpose is to act out as a format that adresses the problem the netowrk is tryin to solve by setting an output vector.\n",
    "- **Activation:** A transformed output fitted int to a format that will fit the function.\n",
    "- **Backpropagation:** The use of weights from a neural network and gradient decent to calculate the error function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q2\"></a>\n",
    "\n",
    "The XOr, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOr logic gates given two binary inputs. An XOr function should return a true value if the two inputs are not equal and a false value if they are equal. Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2 | y |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "| 1 | 0 | 1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   1  0\n",
       "3   1   0  1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Data\n",
    "data = { 'x1': [0,0,1,1],\n",
    "        'x2': [0,1,1,0],\n",
    "        'y':  [0,1,0,1]\n",
    "      }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 0]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "#Np Array tranform\n",
    "X = np.array(df[['x1', 'x2']].values)\n",
    "Y=np.array(df[['y']].values)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Perceptron\n",
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron estimator with early stopping.\n",
    "    \n",
    "    :param learning_rate: float Estimator learning rate. Default == 0.01\n",
    "    :param epochs: int Number of epochs to run Perceptron. Default = 1000\n",
    "    :param early_stopping: int Number of epochs without imoprovement at which to stop estimator. Default = 10\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, epochs=100, early_stopping=10):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "    def predict(self,row):\n",
    "        \"\"\"Apply weights and add bias to inputs.\n",
    "        \n",
    "        Return 1 if output is greater or equal zero, else zero for each element in input row.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (np.dot(row, self.weight[1:]) + self.weight[0]) >= 0\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit training data\n",
    "        \n",
    "        Initialize with random bias and weights.\n",
    "        Update weights and bias with each row based on previous iteration's error.\n",
    "        Store number of errors for each epoch.\n",
    "        Stop if no errors in number of `early_stopping` epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weight = np.array([np.random.random() for _ in range(x.shape[1] + 1)])\n",
    "    \n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            error = 0\n",
    "            for row, label in zip(x, y):\n",
    "                \n",
    "                # Check our current prediction against the actual label to get the error.\n",
    "                # Multiply the result by the learning rate.\n",
    "                adjustment = self.lr * (label - self.predict(row))\n",
    "                \n",
    "                # Adjust our weigts and bias accordingly.\n",
    "                self.weight[1:] += adjustment * row\n",
    "                self.weight[0] += adjustment\n",
    "                \n",
    "                # Add up our errors for each epoch.\n",
    "                error += adjustment != 0.0\n",
    "                \n",
    "            # Make a list of number of errors per epoch.\n",
    "            self.errors_.append(error)\n",
    "\n",
    "            # If correct each time for a number of rounds, it will stop already.\n",
    "            if sum(self.errors_[-self.early_stopping:]) == 0:\n",
    "                print('Stopped Early')\n",
    "                break\n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining\n",
    "perceptron = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x11db62950>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting\n",
    "perceptron.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting\n",
    "perceptron.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.,  1.,  3., ...,  0.,  1.,  1.],\n",
       "       [37.,  1.,  2., ...,  0.,  2.,  1.],\n",
       "       [41.,  0.,  1., ...,  0.,  2.,  1.],\n",
       "       ...,\n",
       "       [68.,  1.,  0., ...,  2.,  3.,  0.],\n",
       "       [57.,  1.,  0., ...,  1.,  3.,  0.],\n",
       "       [57.,  0.,  1., ...,  1.,  2.,  0.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv'\n",
    "df1 = pd.read_csv(url).values\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spliting input x and output y variables\n",
    "x = df1[:,0:13]\n",
    "y = df1[:,13]\n",
    "y\n",
    "#numpy arrays\n",
    "#y_new = np.zeros(y.shape)\n",
    "#y_new[np.where(y == 0.0)[0]] = 1\n",
    "#y = y_new\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inputs = 64\n",
    "        self.hiddenNodes = 32\n",
    "        self.outputNodes = 1\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        self.hidden_sum = np.dot(x, self.weights1)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, x, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        self.weights1 += x.T.dot(self.z2_delta)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        o = self.feed_forward(x)\n",
    "        self.backward(x, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[0.03913132 0.34757345 0.08690453 ... 0.13281103 0.46536008 0.01050626]\n",
      " [0.91425982 0.24125672 0.93197617 ... 0.70611477 0.77145759 0.85408045]\n",
      " [0.57981084 0.87925799 0.44517593 ... 0.52468816 0.57462314 0.99874055]\n",
      " ...\n",
      " [0.46508551 0.60322434 0.82282193 ... 0.30443618 0.16556039 0.5390551 ]\n",
      " [0.61059336 0.76455655 0.52668948 ... 0.67722378 0.49364761 0.50307098]\n",
      " [0.85351715 0.12929479 0.14213341 ... 0.11210744 0.09914696 0.2209307 ]]\n",
      "Layer 2 wieghts: \n",
      " [[0.91904196]\n",
      " [0.91251437]\n",
      " [0.08468065]\n",
      " [0.31731484]\n",
      " [0.9821451 ]\n",
      " [0.53666093]\n",
      " [0.50283707]\n",
      " [0.9775469 ]\n",
      " [0.64876207]\n",
      " [0.05026465]\n",
      " [0.90095428]\n",
      " [0.86710402]\n",
      " [0.36559286]\n",
      " [0.54881564]\n",
      " [0.6403375 ]\n",
      " [0.72586929]\n",
      " [0.6763298 ]\n",
      " [0.11914422]\n",
      " [0.23903809]\n",
      " [0.2138331 ]\n",
      " [0.95705823]\n",
      " [0.30526774]\n",
      " [0.07354595]\n",
      " [0.65406108]\n",
      " [0.21777952]\n",
      " [0.3740138 ]\n",
      " [0.64504081]\n",
      " [0.9261937 ]\n",
      " [0.93400441]\n",
      " [0.64048911]\n",
      " [0.66318737]\n",
      " [0.77223021]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 wieghts: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward\n",
      " <bound method NeuralNetwork.backward of <__main__.NeuralNetwork object at 0x14bf46210>> \n",
      "---------\n",
      "feed_forward\n",
      " <bound method NeuralNetwork.feed_forward of <__main__.NeuralNetwork object at 0x14bf46210>> \n",
      "---------\n",
      "hiddenNodes\n",
      " 32 \n",
      "---------\n",
      "inputs\n",
      " 64 \n",
      "---------\n",
      "outputNodes\n",
      " 1 \n",
      "---------\n",
      "sigmoid\n",
      " <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x14bf46210>> \n",
      "---------\n",
      "sigmoidPrime\n",
      " <bound method NeuralNetwork.sigmoidPrime of <__main__.NeuralNetwork object at 0x14bf46210>> \n",
      "---------\n",
      "train\n",
      " <bound method NeuralNetwork.train of <__main__.NeuralNetwork object at 0x14bf46210>> \n",
      "---------\n",
      "weights1\n",
      " [[0.70436293 0.89890989 0.56053191 ... 0.82045266 0.4895408  0.06965122]\n",
      " [0.19226044 0.31257819 0.74418144 ... 0.59922983 0.38919547 0.77112528]\n",
      " [0.96984058 0.19574207 0.40281323 ... 0.51449472 0.96546536 0.30555961]\n",
      " ...\n",
      " [0.65986452 0.06609291 0.71074347 ... 0.05653763 0.82942011 0.34260953]\n",
      " [0.99486099 0.63325643 0.41733619 ... 0.62598943 0.77310389 0.56288828]\n",
      " [0.7303045  0.30205833 0.92907596 ... 0.44339408 0.97132164 0.99453524]] \n",
      "---------\n",
      "weights2\n",
      " [[0.16190333]\n",
      " [0.15713425]\n",
      " [0.68406895]\n",
      " [0.01556027]\n",
      " [0.22327188]\n",
      " [0.05377006]\n",
      " [0.4272095 ]\n",
      " [0.93073564]\n",
      " [0.22063849]\n",
      " [0.88512527]\n",
      " [0.25554631]\n",
      " [0.94907283]\n",
      " [0.93284507]\n",
      " [0.40263582]\n",
      " [0.1966374 ]\n",
      " [0.77722243]\n",
      " [0.47216548]\n",
      " [0.96014482]\n",
      " [0.30349848]\n",
      " [0.90311414]\n",
      " [0.70187113]\n",
      " [0.55453233]\n",
      " [0.02183337]\n",
      " [0.61594469]\n",
      " [0.89537239]\n",
      " [0.68950355]\n",
      " [0.97999243]\n",
      " [0.94374546]\n",
      " [0.02479564]\n",
      " [0.58703762]\n",
      " [0.67834947]\n",
      " [0.42504844]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'output+']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in dir(nn) if i[:2]!= '__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv'\n",
    "df2 = pd.read_csv(url)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2['target'].values\n",
    "x = df2.drop('target', axis = 'columns').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Fit the model\n",
    "    return model\n",
    "\n",
    "#Keras Model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcalimayor/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 12ms/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 420us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 435us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 470us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 621us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 590us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 432us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 377us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 434us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 508us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 531us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 490us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 433us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 404us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 392us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 457us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 354us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 326us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 340us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 375us/step - loss: 10.8913 - acc: 0.3168\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 2.5186 - acc: 0.6089\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 297us/step - loss: 1.1080 - acc: 0.6485\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 305us/step - loss: 1.0223 - acc: 0.6634\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 309us/step - loss: 1.0009 - acc: 0.6485\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 313us/step - loss: 1.0012 - acc: 0.6485\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 364us/step - loss: 0.9781 - acc: 0.6337\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 417us/step - loss: 0.9385 - acc: 0.6089\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 330us/step - loss: 0.8845 - acc: 0.6337\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 325us/step - loss: 0.9275 - acc: 0.6436\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 348us/step - loss: 0.8819 - acc: 0.6188\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 367us/step - loss: 0.8545 - acc: 0.6386\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 378us/step - loss: 0.8191 - acc: 0.6485\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 337us/step - loss: 0.8069 - acc: 0.6733\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 323us/step - loss: 0.7790 - acc: 0.6584\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 325us/step - loss: 0.7560 - acc: 0.6436\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 338us/step - loss: 0.7351 - acc: 0.7030\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 327us/step - loss: 0.7768 - acc: 0.6485\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 407us/step - loss: 0.7176 - acc: 0.6485\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 323us/step - loss: 0.7084 - acc: 0.6733\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 323us/step - loss: 0.7054 - acc: 0.7079\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 1.1328 - acc: 0.7228\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 594us/step - loss: 1.0113 - acc: 0.7327\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 542us/step - loss: 0.8742 - acc: 0.7574\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 483us/step - loss: 0.8390 - acc: 0.7574\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 468us/step - loss: 0.8099 - acc: 0.7376\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 488us/step - loss: 0.7337 - acc: 0.7723\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 458us/step - loss: 0.7407 - acc: 0.7673\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 545us/step - loss: 0.6772 - acc: 0.8119\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 517us/step - loss: 0.6604 - acc: 0.8020\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 699us/step - loss: 0.6388 - acc: 0.7673\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 550us/step - loss: 0.6403 - acc: 0.7822\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 552us/step - loss: 0.6280 - acc: 0.7673\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 518us/step - loss: 0.5662 - acc: 0.7822\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 530us/step - loss: 0.6004 - acc: 0.8020\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 543us/step - loss: 0.5726 - acc: 0.7871\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 529us/step - loss: 0.5384 - acc: 0.7772\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 557us/step - loss: 0.5127 - acc: 0.8267\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 470us/step - loss: 0.5083 - acc: 0.7772\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 431us/step - loss: 0.5241 - acc: 0.7822\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 501us/step - loss: 0.5056 - acc: 0.8119\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 264us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 247us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 239us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 388us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 248us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 235us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 249us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 227us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 253us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 277us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 272us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 272us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 253us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 249us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 232us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 242us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 235us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 205us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 257us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 16ms/step - loss: 2.1908 - acc: 0.5149\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 176us/step - loss: 1.3004 - acc: 0.5446\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 199us/step - loss: 1.1964 - acc: 0.5644\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 236us/step - loss: 1.1575 - acc: 0.5050\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 229us/step - loss: 1.0853 - acc: 0.5545\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 221us/step - loss: 1.0379 - acc: 0.5495\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 180us/step - loss: 0.9897 - acc: 0.5149\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 219us/step - loss: 0.9131 - acc: 0.5495\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 265us/step - loss: 0.9322 - acc: 0.5891\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 258us/step - loss: 0.8828 - acc: 0.5396\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 263us/step - loss: 0.8266 - acc: 0.5644\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 244us/step - loss: 0.8340 - acc: 0.5594\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 297us/step - loss: 0.8006 - acc: 0.5743\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 259us/step - loss: 0.7793 - acc: 0.5792\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 396us/step - loss: 0.7809 - acc: 0.6040\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 919us/step - loss: 0.7330 - acc: 0.5792\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 393us/step - loss: 0.7238 - acc: 0.6089\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 368us/step - loss: 0.7360 - acc: 0.5842\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 417us/step - loss: 0.7129 - acc: 0.5792\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 266us/step - loss: 0.6795 - acc: 0.6139\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 17ms/step - loss: 1.2536 - acc: 0.4851\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 208us/step - loss: 0.4742 - acc: 0.7921\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 231us/step - loss: 0.4598 - acc: 0.8119\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 401us/step - loss: 0.4536 - acc: 0.8317\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 244us/step - loss: 0.4577 - acc: 0.7871\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 221us/step - loss: 0.4578 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 225us/step - loss: 0.4556 - acc: 0.8267\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 231us/step - loss: 0.4461 - acc: 0.8366\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 196us/step - loss: 0.4627 - acc: 0.8020\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 234us/step - loss: 0.4371 - acc: 0.8267\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 217us/step - loss: 0.4320 - acc: 0.8515\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 225us/step - loss: 0.4346 - acc: 0.8218\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 240us/step - loss: 0.4414 - acc: 0.8267\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 277us/step - loss: 0.4345 - acc: 0.8267\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 303us/step - loss: 0.4339 - acc: 0.8317\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 230us/step - loss: 0.4345 - acc: 0.8317\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 253us/step - loss: 0.4219 - acc: 0.8465\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 339us/step - loss: 0.4280 - acc: 0.8267\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 468us/step - loss: 0.4260 - acc: 0.8267\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 246us/step - loss: 0.4266 - acc: 0.8366\n",
      "101/101 [==============================] - 1s 9ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 16ms/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 158us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 125us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 139us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 142us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 129us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 137us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 141us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 135us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 138us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 124us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 242us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 369us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 233us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 605us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 483us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 216us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 288us/step - loss: 10.8913 - acc: 0.3168\n",
      "101/101 [==============================] - 1s 9ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 4.7673 - acc: 0.3317\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 125us/step - loss: 3.4389 - acc: 0.3267\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 173us/step - loss: 2.5745 - acc: 0.3020\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 144us/step - loss: 2.0534 - acc: 0.3366\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 123us/step - loss: 1.7254 - acc: 0.3614\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 150us/step - loss: 1.9894 - acc: 0.3564\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 191us/step - loss: 1.7214 - acc: 0.4208\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 252us/step - loss: 1.6494 - acc: 0.4307\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 179us/step - loss: 1.2028 - acc: 0.4455\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 257us/step - loss: 1.3975 - acc: 0.4653\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 215us/step - loss: 1.3644 - acc: 0.5248\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 171us/step - loss: 1.0741 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 166us/step - loss: 1.2522 - acc: 0.5297\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 141us/step - loss: 1.0437 - acc: 0.5099\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 165us/step - loss: 1.2683 - acc: 0.5198\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 174us/step - loss: 1.0085 - acc: 0.5495\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 158us/step - loss: 1.1099 - acc: 0.5545\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 182us/step - loss: 1.0424 - acc: 0.5594\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 172us/step - loss: 1.0122 - acc: 0.4950\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 166us/step - loss: 1.0072 - acc: 0.5693\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 16ms/step - loss: 2.0676 - acc: 0.7624\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 1.5772 - acc: 0.6584\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 124us/step - loss: 1.6049 - acc: 0.7178\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 140us/step - loss: 1.3793 - acc: 0.7228\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 132us/step - loss: 1.5166 - acc: 0.5495\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 168us/step - loss: 1.2167 - acc: 0.7079\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 189us/step - loss: 1.2370 - acc: 0.7525\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 365us/step - loss: 1.1849 - acc: 0.6535\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 331us/step - loss: 1.1422 - acc: 0.7822\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 326us/step - loss: 0.9765 - acc: 0.7723\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 257us/step - loss: 0.8900 - acc: 0.7079\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 336us/step - loss: 0.7307 - acc: 0.7921\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 182us/step - loss: 0.6229 - acc: 0.7921\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 248us/step - loss: 0.7481 - acc: 0.6535\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 282us/step - loss: 0.5595 - acc: 0.8218\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 545us/step - loss: 0.4746 - acc: 0.8267\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 397us/step - loss: 0.5377 - acc: 0.8218\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 214us/step - loss: 0.6825 - acc: 0.6881\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 186us/step - loss: 0.4522 - acc: 0.8218\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 338us/step - loss: 0.5170 - acc: 0.8069\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 15ms/step - loss: 6.1658 - acc: 0.3317\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 89us/step - loss: 0.8806 - acc: 0.5594\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 0.8264 - acc: 0.6089\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 111us/step - loss: 0.8203 - acc: 0.5792\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 124us/step - loss: 0.7867 - acc: 0.6634\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 0.7354 - acc: 0.6337\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 92us/step - loss: 0.7366 - acc: 0.6634\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 0.7073 - acc: 0.6535\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 94us/step - loss: 0.6887 - acc: 0.6782\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 135us/step - loss: 0.7905 - acc: 0.6089\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 97us/step - loss: 0.7085 - acc: 0.6485\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.6293 - acc: 0.6980\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 124us/step - loss: 0.6397 - acc: 0.6931\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.6423 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 96us/step - loss: 0.7063 - acc: 0.6584\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 0.6409 - acc: 0.6881\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 157us/step - loss: 0.5903 - acc: 0.7129\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 0.5892 - acc: 0.7030\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 97us/step - loss: 0.5684 - acc: 0.7475\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 0.6307 - acc: 0.6782\n",
      "101/101 [==============================] - 1s 9ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 15ms/step - loss: 3.8077 - acc: 0.4604\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 2.8984 - acc: 0.4604\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 2.3093 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 1.8024 - acc: 0.5248\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 1.5439 - acc: 0.5495\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 82us/step - loss: 1.3752 - acc: 0.5594\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 1.1803 - acc: 0.5792\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 1.0163 - acc: 0.5941\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.9274 - acc: 0.6238\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 82us/step - loss: 0.9105 - acc: 0.6287\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 127us/step - loss: 0.8997 - acc: 0.6634\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.7845 - acc: 0.6733\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 87us/step - loss: 0.7661 - acc: 0.6733\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 0.7549 - acc: 0.6782\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 127us/step - loss: 0.7784 - acc: 0.6683\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 93us/step - loss: 0.7076 - acc: 0.6980\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 84us/step - loss: 0.6858 - acc: 0.7277\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 93us/step - loss: 0.6855 - acc: 0.7079\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 130us/step - loss: 0.6918 - acc: 0.7079\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 98us/step - loss: 0.6683 - acc: 0.7030\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 15ms/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 82us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 84us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 111us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 99us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 89us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 85us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 92us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 149us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 88us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 119us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 230us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 144us/step - loss: 2.9201 - acc: 0.8168\n",
      "101/101 [==============================] - 1s 9ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 9.6134 - acc: 0.3218\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 7.6145 - acc: 0.3663\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 5.2567 - acc: 0.3960\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 3.4654 - acc: 0.4653\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 2.3789 - acc: 0.5149\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 96us/step - loss: 1.7053 - acc: 0.5594\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 1.3195 - acc: 0.5792\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 67us/step - loss: 1.0667 - acc: 0.6040\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.9050 - acc: 0.5990\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 83us/step - loss: 0.8041 - acc: 0.5990\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.7486 - acc: 0.5990\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.7113 - acc: 0.6139\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 0.6786 - acc: 0.6485\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.6561 - acc: 0.6634\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 70us/step - loss: 0.6371 - acc: 0.6683\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 93us/step - loss: 0.6427 - acc: 0.6881\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.6162 - acc: 0.7129\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 0.6119 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.6086 - acc: 0.6881\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 78us/step - loss: 0.6027 - acc: 0.7129\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 71us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 57us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 95us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 78us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 73us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 7.9712 - acc: 0.5000\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 15ms/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 70us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 73us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 73us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 73us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 72us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 88us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 182us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 147us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 133us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 83us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 13.1658 - acc: 0.1832\n",
      "101/101 [==============================] - 2s 16ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 5s 27ms/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 71us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 70us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 72us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 5s 24ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 200us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 131us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 173us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 190us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 124us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 128us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 8.0590 - acc: 0.5000\n",
      "101/101 [==============================] - 2s 21ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 6s 28ms/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 82us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 67us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 2.9201 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 73us/step - loss: 2.9201 - acc: 0.8168\n",
      "101/101 [==============================] - 2s 17ms/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 5s 16ms/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 337us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 382us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 359us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 356us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 366us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 515us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 333us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 335us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 332us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 404us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 721us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 348us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 801us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 798us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 919us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 465us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 467us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 711us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 725us/step - loss: 7.2609 - acc: 0.5446\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "#Creating Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "202/202 [==============================] - 4s 17ms/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 163us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 98us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 94us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 89us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 124us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 112us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 17ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 104us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 134us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 93us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 112us/step - loss: 8.0590 - acc: 0.5000\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 16ms/step - loss: 2.1398 - acc: 0.7921\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 1.5406 - acc: 0.7376\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 1.3599 - acc: 0.6980\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 98us/step - loss: 1.1380 - acc: 0.7574\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 1.2572 - acc: 0.6980\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 138us/step - loss: 1.0211 - acc: 0.7327\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 129us/step - loss: 0.9808 - acc: 0.7277\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.9799 - acc: 0.7277\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 111us/step - loss: 0.9653 - acc: 0.7574\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 0.9299 - acc: 0.7426\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 143us/step - loss: 0.8811 - acc: 0.7673\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 0.8937 - acc: 0.7475\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 134us/step - loss: 0.9124 - acc: 0.7624\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 132us/step - loss: 0.8367 - acc: 0.7871\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 0.9309 - acc: 0.7327\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.8067 - acc: 0.7525\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.9379 - acc: 0.7723\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 0.7895 - acc: 0.7426\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 113us/step - loss: 0.8100 - acc: 0.7475\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 0.7626 - acc: 0.7525\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 5s 15ms/step - loss: 2.6267 - acc: 0.4752\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 83us/step - loss: 1.7202 - acc: 0.4818\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 81us/step - loss: 1.4993 - acc: 0.4950\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 118us/step - loss: 1.4768 - acc: 0.4917\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 184us/step - loss: 1.3174 - acc: 0.5314\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 339us/step - loss: 1.1302 - acc: 0.4950\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 200us/step - loss: 1.1678 - acc: 0.5545\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 132us/step - loss: 0.9908 - acc: 0.5149\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 136us/step - loss: 1.0997 - acc: 0.5248\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 138us/step - loss: 0.8769 - acc: 0.5512\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 327us/step - loss: 0.8994 - acc: 0.5809\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 220us/step - loss: 1.0354 - acc: 0.5578\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 213us/step - loss: 0.9344 - acc: 0.5314\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 213us/step - loss: 0.7584 - acc: 0.5677\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 158us/step - loss: 0.7510 - acc: 0.5908\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 154us/step - loss: 0.8260 - acc: 0.5578\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 206us/step - loss: 0.8210 - acc: 0.6007\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 179us/step - loss: 0.8292 - acc: 0.6139\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 164us/step - loss: 0.7496 - acc: 0.6040\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 154us/step - loss: 0.7602 - acc: 0.6271\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [60],\n",
    "              'epochs': [20]}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "202/202 [==============================] - 4s 19ms/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 130us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 122us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 142us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 115us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 130us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 128us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 97us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 99us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 119us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 220us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 116us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 113us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 94us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 158us/step - loss: 5.1067 - acc: 0.6832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 136us/step - loss: 5.1067 - acc: 0.6832\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 4s 21ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 194us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 127us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 101us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 112us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 167us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 118us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 112us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 233us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 183us/step - loss: 8.0590 - acc: 0.5000\n",
      "101/101 [==============================] - 2s 17ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 4s 18ms/step - loss: 1.9273 - acc: 0.7673\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 1.5910 - acc: 0.7673\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 1.4288 - acc: 0.7525\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 1.3873 - acc: 0.7475\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 1.1826 - acc: 0.7525\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 1.0957 - acc: 0.7673\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 1.0349 - acc: 0.7426\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 94us/step - loss: 0.9253 - acc: 0.7822\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 0.9002 - acc: 0.7574\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.8414 - acc: 0.7525\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 0.8166 - acc: 0.7624\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 91us/step - loss: 0.7527 - acc: 0.7574\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 111us/step - loss: 0.7469 - acc: 0.7822\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.7345 - acc: 0.7723\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 78us/step - loss: 0.7543 - acc: 0.7723\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 84us/step - loss: 0.7528 - acc: 0.7525\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 87us/step - loss: 0.6885 - acc: 0.7822\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 0.7133 - acc: 0.7624\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.7178 - acc: 0.7574\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 96us/step - loss: 0.6699 - acc: 0.7574\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 4s 17ms/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 91us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 67us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 67us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 70us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 10.8913 - acc: 0.3168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 10.8913 - acc: 0.3168\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 17ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 67us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 85us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 61us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 72us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 72us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 70us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 71us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 8.0590 - acc: 0.5000\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 4s 17ms/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 72us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 70us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 66us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 68us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 65us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 69us/step - loss: 13.1658 - acc: 0.1832\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 64us/step - loss: 13.1658 - acc: 0.1832\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 4s 12ms/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 54us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 54us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 54us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 54us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 58us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 53us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 56us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 58us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 55us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 70us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 60us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 60us/step - loss: 7.2609 - acc: 0.5446\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 59us/step - loss: 7.2609 - acc: 0.5446\n",
      "Best: 0.7887788786746488 using {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.18811880794688812, Stdev: 0.14972007790057856 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7887788786746488, Stdev: 0.29871177443796465 with: {'batch_size': 80, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [60, 80],\n",
    "              'epochs': [20]}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(x, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "danlol",
   "language": "python",
   "name": "danlol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
